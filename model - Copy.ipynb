{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "from skimage.feature import hog\n",
    "from skimage import color\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>img_file</th>\n",
       "      <th>classification</th>\n",
       "      <th>no_of_contours</th>\n",
       "      <th>processed_img</th>\n",
       "      <th>diagnosis</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>./dataset/train_img/raw_img/intra1.jpg</td>\n",
       "      <td>healthy</td>\n",
       "      <td>2</td>\n",
       "      <td>./dataset/train_img/processed_img/closed_img/i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>./dataset/train_img/raw_img/intra2.jpg</td>\n",
       "      <td>gingivitis</td>\n",
       "      <td>5</td>\n",
       "      <td>./dataset/train_img/processed_img/closed_img/i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>./dataset/train_img/raw_img/intra3.jpg</td>\n",
       "      <td>healthy</td>\n",
       "      <td>2</td>\n",
       "      <td>./dataset/train_img/processed_img/closed_img/i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>./dataset/train_img/raw_img/intra4.jpg</td>\n",
       "      <td>healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>./dataset/train_img/processed_img/closed_img/i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>./dataset/train_img/raw_img/intra5.jpg</td>\n",
       "      <td>healthy</td>\n",
       "      <td>0</td>\n",
       "      <td>./dataset/train_img/processed_img/closed_img/i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  img_file classification  no_of_contours  \\\n",
       "id                                                                          \n",
       "1   ./dataset/train_img/raw_img/intra1.jpg        healthy               2   \n",
       "2   ./dataset/train_img/raw_img/intra2.jpg     gingivitis               5   \n",
       "3   ./dataset/train_img/raw_img/intra3.jpg        healthy               2   \n",
       "4   ./dataset/train_img/raw_img/intra4.jpg        healthy               0   \n",
       "5   ./dataset/train_img/raw_img/intra5.jpg        healthy               0   \n",
       "\n",
       "                                        processed_img  diagnosis  \n",
       "id                                                                \n",
       "1   ./dataset/train_img/processed_img/closed_img/i...          0  \n",
       "2   ./dataset/train_img/processed_img/closed_img/i...          1  \n",
       "3   ./dataset/train_img/processed_img/closed_img/i...          0  \n",
       "4   ./dataset/train_img/processed_img/closed_img/i...          0  \n",
       "5   ./dataset/train_img/processed_img/closed_img/i...          0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = pd.read_csv('./dataset/dataset.csv', index_col=0)\n",
    "labels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_processedimg(row_id, root=\"./dataset/train_img/processed_img/noise_reduced_img\"):\n",
    "\n",
    "    filename = \"intra{}.jpg\".format(row_id)\n",
    "    file_path = os.path.join(root, filename)\n",
    "    img = Image.open(file_path)\n",
    "    return np.array(img)\n",
    "\n",
    "def get_img(row_id, root=\"./dataset/train_img/raw_img/\"):\n",
    "\n",
    "    filename = \"intra{}.jpg\".format(row_id)\n",
    "    file_path = os.path.join(root, filename)\n",
    "    img = Image.open(file_path)\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# subset the dataframe to just healthy (diagnosis is 0.0) get the value of the sixth item in the index\n",
    "healthy_row = labels[labels.diagnosis == 0.0].index[5]\n",
    "\n",
    "# show the processed image of an healthy gum\n",
    "plt.imshow(get_processedimg(healthy_row))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# subset the dataframe to just gingivitis (diagnosis is 1.0) get the value of the sixth item in the index\n",
    "gingivitis_row = labels[labels.diagnosis == 1.0].index[5]\n",
    "\n",
    "# show the processed image of an infected gum/gingivitis\n",
    "plt.imshow(get_processedimg(gingivitis_row))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Color gingivitis image has shape:  [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "gingi = get_processedimg(gingivitis_row)\n",
    "\n",
    "print('Color gingivitis image has shape: ', gingi)\n",
    "\n",
    "\n",
    "plt.imshow(gingi, cmap=mpl.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Greyscale gingivitis image has shape:  [[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print('Greyscale gingivitis image has shape: ', gingi)\n",
    "\n",
    "hog_features, hog_image = hog(gingi,\n",
    "                              visualize=True,\n",
    "                              block_norm='L2-Hys',\n",
    "                              pixels_per_cell=(16, 16))\n",
    "\n",
    "plt.imshow(hog_image, cmap=mpl.cm.gray)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "def create_features(img):\n",
    "    # flatten three channel color image\n",
    "    color_features = img.flatten()\n",
    "    # get HOG features from greyscale image\n",
    "    hog_features = hog(img, block_norm='L2-Hys', pixels_per_cell=(16, 16))\n",
    "    # combine color and hog features into a single array\n",
    "    flat_features = np.hstack(color_features)\n",
    "    return flat_features\n",
    "\n",
    "gingi_features = create_features(gingi)\n",
    "\n",
    "print(gingi_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_matrix(label_dataframe):\n",
    "    features_list = []\n",
    "    \n",
    "    for img_id in label_dataframe.index:\n",
    "        # load image\n",
    "        img = get_processedimg(img_id)\n",
    "        # get features for image\n",
    "        image_features = create_features(img)\n",
    "        features_list.append(image_features)\n",
    "        \n",
    "    # convert list of arrays into a matrix\n",
    "    feature_matrix = np.array(features_list)\n",
    "    return feature_matrix\n",
    "\n",
    "# run create_feature_matrix on our dataframe of images\n",
    "feature_matrix = create_feature_matrix(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature matrix shape is:  (417, 65536)\n",
      "PCA matrix shape is:  (417, 65536)\n"
     ]
    }
   ],
   "source": [
    "# get shape of feature matrix\n",
    "print('Feature matrix shape is: ', feature_matrix.shape)\n",
    "\n",
    "# define standard scaler\n",
    "ss = StandardScaler()\n",
    "# run this on our feature matrix\n",
    "bees_stand = ss.fit_transform(feature_matrix)\n",
    "\n",
    "pca = PCA(n_components=500)\n",
    "# use fit_transform to run PCA on our standardized matrix\n",
    "gum_pca = ss.fit_transform(bees_stand)\n",
    "# look at new shape\n",
    "print('PCA matrix shape is: ', gum_pca.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    179\n",
       "0    112\n",
       "dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pd.DataFrame(gum_pca)\n",
    "y = pd.Series(labels.diagnosis.values)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=.3,\n",
    "                                                    random_state=1234123)\n",
    "\n",
    "# look at the distrubution of labels in the train set\n",
    "pd.Series(y_train).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(291, 65536)\n",
      "(291,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "3/3 [==============================] - 60s 16s/step - loss: 1.6652 - accuracy: 0.5603 - val_loss: 1.2330 - val_accuracy: 0.5085\n",
      "Epoch 2/5\n",
      "3/3 [==============================] - 21s 6s/step - loss: 0.6854 - accuracy: 0.5259 - val_loss: 1.2300 - val_accuracy: 0.4915\n",
      "Epoch 3/5\n",
      "3/3 [==============================] - 14s 4s/step - loss: 0.5394 - accuracy: 0.5862 - val_loss: 1.7506 - val_accuracy: 0.5593\n",
      "Epoch 4/5\n",
      "3/3 [==============================] - 14s 5s/step - loss: 0.5092 - accuracy: 0.6250 - val_loss: 2.3286 - val_accuracy: 0.5932\n",
      "Epoch 5/5\n",
      "3/3 [==============================] - 13s 4s/step - loss: 0.4986 - accuracy: 0.6983 - val_loss: 2.8629 - val_accuracy: 0.5932\n",
      "4/4 [==============================] - 2s 419ms/step - loss: 2.1702 - accuracy: 0.5714\n",
      "Test accuracy: 0.5714285969734192\n",
      "4/4 [==============================] - 4s 430ms/step\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.20      0.02      0.04        51\n",
      "           1       0.59      0.95      0.72        75\n",
      "\n",
      "    accuracy                           0.57       126\n",
      "   macro avg       0.39      0.48      0.38       126\n",
      "weighted avg       0.43      0.57      0.45       126\n",
      "\n",
      "0.5714285714285714\n",
      "[[ 1 50]\n",
      " [ 4 71]]\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras import regularizers\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "# Reshape the input data into 2D images\n",
    "\n",
    "arr = np.array(X_train).reshape(X_train.shape[0], -1)\n",
    "X_train = np.array(X_train).reshape((X_train.shape[0], 256, 256, 1))\n",
    "\n",
    "# Define the model architecture\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(256, 256, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=97, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "X_test = np.array(X_test).reshape((X_test.shape[0], 256, 256, 1))\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(\"Test accuracy:\", test_acc)\n",
    "predictions = model.predict(X_test)\n",
    "predictions=[0 if val <0.5 else 1 for val in predictions]\n",
    "print(predictions)\n",
    "print(classification_report(y_test, predictions))\n",
    "print(accuracy_score(y_test, predictions))\n",
    "print(confusion_matrix(y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n",
       "                       max_depth=None, max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is:  0.5873015873015873\n"
     ]
    }
   ],
   "source": [
    "y_pred = dt.predict(X_test)\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Model accuracy is: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is:  0.5873015873015873\n"
     ]
    }
   ],
   "source": [
    "print('Model accuracy is: ', dt.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                       criterion='gini', max_depth=None, max_features='auto',\n",
       "                       max_leaf_nodes=None, max_samples=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, n_estimators=20,\n",
       "                       n_jobs=None, oob_score=False, random_state=None,\n",
       "                       verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn. ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier(n_estimators=20)\n",
    "rf.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5793650793650794"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = rf.predict(X_test)\n",
    "rf.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
       "    decision_function_shape='ovr', degree=3, gamma='scale', kernel='linear',\n",
       "    max_iter=-1, probability=True, random_state=42, shrinking=True, tol=0.001,\n",
       "    verbose=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define support vector classifier\n",
    "svm = SVC(kernel='linear', probability=True, random_state=42)\n",
    "\n",
    "# fit model\n",
    "svm.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model accuracy is:  0.5714285714285714\n"
     ]
    }
   ],
   "source": [
    "# generate predictions\n",
    "y_pred = svm.predict(X_test)\n",
    "\n",
    "# calculate accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print('Model accuracy is: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "10/10 [==============================] - 1s 15ms/step - loss: 0.8499 - accuracy: 0.5326\n",
      "Epoch 2/10\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.8192 - accuracy: 0.5670\n",
      "Epoch 3/10\n",
      "10/10 [==============================] - 0s 12ms/step - loss: 0.8178 - accuracy: 0.5670\n",
      "Epoch 4/10\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.8124 - accuracy: 0.5739\n",
      "Epoch 5/10\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.8058 - accuracy: 0.5773\n",
      "Epoch 6/10\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.7972 - accuracy: 0.5876\n",
      "Epoch 7/10\n",
      "10/10 [==============================] - 0s 13ms/step - loss: 0.7942 - accuracy: 0.5911\n",
      "Epoch 8/10\n",
      "10/10 [==============================] - 0s 14ms/step - loss: 0.7852 - accuracy: 0.6014\n",
      "Epoch 9/10\n",
      "10/10 [==============================] - 0s 11ms/step - loss: 0.7808 - accuracy: 0.6048\n",
      "Epoch 10/10\n",
      "10/10 [==============================] - 0s 9ms/step - loss: 0.7780 - accuracy: 0.6082\n",
      "4/4 [==============================] - 0s 13ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.32      0.16      0.21        51\n",
      "           1       0.57      0.77      0.66        75\n",
      "\n",
      "    accuracy                           0.52       126\n",
      "   macro avg       0.45      0.47      0.43       126\n",
      "weighted avg       0.47      0.52      0.48       126\n",
      "\n",
      "0.5238095238095238\n",
      "[[ 8 43]\n",
      " [17 58]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "#input_dim=X_train.shape[1]\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Dense(1, input_dim=len(X_train.columns), activation='sigmoid',\n",
    "                          kernel_constraint=tf.keras.constraints.UnitNorm())\n",
    "])\n",
    "\n",
    "# Compile the model with hinge loss and no metrics\n",
    "model.compile(loss='hinge', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Fit the model to the training data\n",
    "history=model.fit(X_train, y_train, epochs=53   , batch_size=32)\n",
    "\n",
    "predictions = model.predict(X_test)\n",
    "#print(predictions, \" predictions\")\n",
    "predictions=[0 if val <0.5 else 1 for val in predictions]\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    " \n",
    "print(classification_report(y_test, predictions))\n",
    "print(accuracy_score(y_test, predictions, \" accuracy score\"))\n",
    "print(confusion_matrix(y_test, predictions))\n",
    "\n",
    "# Save the model to disk\n",
    "#model.save('svm_model1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#img4 = cv2.imread(\"./dataset/train_img/raw_img/intragingi2.jpg\")\n",
    "#img3 = cv2.resize(img4, (256,256))\n",
    "#cv2.imwrite('./dataset/train_img/raw_img/intragingi2.jpg',img3)\n",
    "img2 = cv2.imread(\"./dataset/train_img/raw_img/intragingi.jpg\")\n",
    "img3 = cv2.resize(img2, (256,256))\n",
    "cv2.imwrite('./dataset/train_img/raw_img/intragingi.jpg',img3)\n",
    "img4 = cv2.imread(\"./dataset/train_img/raw_img/intragingi.jpg\")\n",
    "ORANGE_MIN = np.array([5, 145, 94],np.uint8)\n",
    "ORANGE_MAX = np.array([8, 255, 255],np.uint8)\n",
    "#img=cv2.rotate(img2, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "hsv_img = cv2.cvtColor(img4,cv2.COLOR_BGR2HSV)\n",
    "frame_threshed = cv2.inRange(hsv_img, ORANGE_MIN, ORANGE_MAX)\n",
    "kernel = np.ones((10, 10), np.uint8)\n",
    "closing = cv2.morphologyEx(frame_threshed, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "opening = cv2.morphologyEx(closing, cv2.MORPH_OPEN, kernel)\n",
    "color_features = opening.flatten()\n",
    "flat_features = np.hstack(color_features)\n",
    "ok=flat_features.reshape(1, -1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]]\n",
      "1/1 [==============================] - 0s 59ms/step\n",
      "1.0  predictions\n",
      "[1]  predictions1\n"
     ]
    }
   ],
   "source": [
    "print(ok)\n",
    "predictions1 = model.predict(ok)\n",
    "print(predictions1[0][0], \" predictions\")\n",
    "predictions1=[0 if val <0.5 else 1 for val in predictions1]\n",
    "#if predictions1[0][0]<0.5:\n",
    "    #print('0')\n",
    "#else:\n",
    "    #print('1')\n",
    "print(predictions1, \" predictions1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: C:\\Users\\Ben\\AppData\\Local\\Temp\\tmpiqq6xki_\\assets\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.models import load_model\n",
    "model = load_model('svm_model1.h5')\n",
    "\n",
    "# Convert the model to a TensorFlow Lite model\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "# Save the TensorFlow Lite model to disk\n",
    "with open('svm_model.tflite', 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# Load the TensorFlow Lite model\n",
    "interpreter = tf.lite.Interpreter(model_path='svm_model.tflite')\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "img2 = cv2.imread(\"./dataset/train_img/raw_img/intragingi2.jpg\")\n",
    "img3 = cv2.resize(img2, (256,256))\n",
    "cv2.imwrite('./dataset/train_img/raw_img/intragingi2.jpg',img3)\n",
    "img4 = cv2.imread(\"./dataset/train_img/raw_img/intragingi2.jpg\")\n",
    "ORANGE_MIN = np.array([5, 145, 94],np.uint8)\n",
    "ORANGE_MAX = np.array([8, 255, 255],np.uint8)\n",
    "#img=cv2.rotate(img3, cv2.ROTATE_90_COUNTERCLOCKWISE)\n",
    "hsv_img = cv2.cvtColor(img4,cv2.COLOR_BGR2HSV)\n",
    "\n",
    "\n",
    "\n",
    "frame_threshed = cv2.inRange(hsv_img, ORANGE_MIN, ORANGE_MAX)\n",
    "\n",
    "\n",
    "\n",
    "kernel = np.ones((10, 10), np.uint8)\n",
    "closing = cv2.morphologyEx(frame_threshed, cv2.MORPH_CLOSE, kernel, iterations=1)\n",
    "opening = cv2.morphologyEx(closing, cv2.MORPH_OPEN, kernel)\n",
    "color_features = opening.flatten()\n",
    "flat_features = np.hstack(color_features)\n",
    "ok=flat_features.reshape(1, -1)\n",
    "\n",
    "# Get the input and output tensors\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "#print(output_details[0])\n",
    "#print(ok.astype(np.float32))\n",
    "\n",
    "# Define a new input for prediction\n",
    "\n",
    "\n",
    "# Set the input tensor\n",
    "interpreter.set_tensor(input_details[0]['index'], ok.astype(np.float32))\n",
    "\n",
    "# Run the inference\n",
    "interpreter.invoke()\n",
    "\n",
    "# Get the output tensor and print the prediction\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "if output_data<0.5:\n",
    "    print('0')\n",
    "else:\n",
    "    print('1')\n",
    "#print(output_data)\n",
    "\n",
    "# Print the predicted classes\n",
    "#predicted_classes = np.argmax(output_data, axis=1)\n",
    "#print(predicted_classes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "cb4361252e18a8a38bcf56925350a1d624ffc42d4be43773c4396c03219dd5f7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
